{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# 简单RAG简介\n",
    "\n",
    "检索增强生成（Retrieval-Augmented Generation，RAG）是一种将信息检索与生成式模型结合的混合方法。它通过引入外部知识提升语言模型的表现，从而提高准确性和事实性。\n",
    "\n",
    "在简单RAG流程中，我们遵循以下步骤：\n",
    "\n",
    "1. **数据导入**：加载并预处理文本数据。\n",
    "2. **分块**：将数据拆分为更小的片段，以提升检索效果。\n",
    "3. **嵌入生成**：使用嵌入模型将文本片段转换为数值表示。\n",
    "4. **语义检索**：根据用户查询检索相关片段。\n",
    "5. **响应生成**：基于检索到的文本，使用语言模型生成回答。\n",
    "\n",
    "本Notebook实现了简单RAG方法，对模型响应进行评估，并探索多种改进方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 环境准备\n",
    "我们首先导入必要的库。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:33:46.775300Z",
     "start_time": "2025-05-23T13:33:46.716007Z"
    }
   },
   "source": [
    "import fitz\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from openai import OpenAI"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:33:52.460895Z",
     "start_time": "2025-05-23T13:33:52.382394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从.env文件中加载环境变量\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从PDF文件中提取文本\n",
    "为了实现RAG，我们首先需要文本数据源。本例中，我们使用PyMuPDF库从PDF文件中提取文本。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    从PDF文件中提取文本，并打印前num_chars个字符。\n",
    "\n",
    "    参数:\n",
    "    pdf_path (str): PDF文件路径。\n",
    "\n",
    "    返回:\n",
    "    str: 提取的文本内容。\n",
    "    \"\"\"\n",
    "    # 打开PDF文件\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # 初始化用于存储提取文本的字符串\n",
    "\n",
    "    # 遍历PDF的每一页\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # 获取当前页\n",
    "        text = page.get_text(\"text\")  # 提取当前页的文本\n",
    "        all_text += text  # 将提取的文本追加到all_text中\n",
    "\n",
    "    return all_text  # 返回提取的文本"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 对提取的文本进行分块\n",
    "获得提取的文本后，我们将其划分为较小且有重叠的片段，以提升检索准确性。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    将文本分割为长度为n、重叠为overlap的片段。\n",
    "\n",
    "    参数:\n",
    "    text (str): 需要分块的文本。\n",
    "    n (int): 每个片段的字符数。\n",
    "    overlap (int): 相邻片段之间的重叠字符数。\n",
    "\n",
    "    返回:\n",
    "    List[str]: 分块后的文本列表。\n",
    "    \"\"\"\n",
    "    chunks = []  # 初始化用于存储片段的列表\n",
    "    \n",
    "    # 以步长(n - overlap)遍历文本\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 从i到i+n切片并加入chunks\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  # 返回分块后的文本列表"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 配置OpenAI API客户端\n",
    "我们初始化OpenAI客户端，用于生成嵌入和回答。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 使用基础URL和API密钥初始化OpenAI客户端\n",
    "client = OpenAI(\n",
    "    base_url=\"https://api.studio.nebius.com/v1/\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # 从环境变量获取API密钥\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 从PDF文件提取并分块文本\n",
    "现在，我们加载PDF，提取文本并将其分块。"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:42:01.844184Z",
     "start_time": "2025-05-23T13:42:01.731941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义PDF文件路径\n",
    "pdf_path = \"data/AI_Information.pdf\"\n",
    "\n",
    "# 从PDF文件中提取文本\n",
    "extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "# 将提取的文本分割为长度1000、重叠200的片段\n",
    "text_chunks = chunk_text(extracted_text, 1000, 200)\n",
    "\n",
    "# 打印分块数量\n",
    "print(\"Number of text chunks:\", len(text_chunks))\n",
    "\n",
    "# 打印第一个文本片段\n",
    "print(\"\\nFirst text chunk:\")\n",
    "print(text_chunks[0])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks: 42\n",
      "\n",
      "First text chunk:\n",
      "Understanding Artificial Intelligence \n",
      "Chapter 1: Introduction to Artificial Intelligence \n",
      "Artificial intelligence (AI) refers to the ability of a digital computer or computer-controlled robot \n",
      "to perform tasks commonly associated with intelligent beings. The term is frequently applied to \n",
      "the project of developing systems endowed with the intellectual processes characteristic of \n",
      "humans, such as the ability to reason, discover meaning, generalize, or learn from past \n",
      "experience. Over the past few decades, advancements in computing power and data availability \n",
      "have significantly accelerated the development and deployment of AI. \n",
      "Historical Context \n",
      "The idea of artificial intelligence has existed for centuries, often depicted in myths and fiction. \n",
      "However, the formal field of AI research began in the mid-20th century. The Dartmouth Workshop \n",
      "in 1956 is widely considered the birthplace of AI. Early AI research focused on problem-solving \n",
      "and symbolic methods. The 1980s saw a rise in exp\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 为文本片段生成嵌入\n",
    "嵌入将文本转换为数值向量，便于高效的相似度检索。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_embeddings(text, model=\"BAAI/bge-en-icl\"):\n",
    "    \"\"\"\n",
    "    使用指定的OpenAI模型为文本生成嵌入。\n",
    "\n",
    "    参数:\n",
    "    text (str): 需要生成嵌入的文本。\n",
    "    model (str): 用于生成嵌入的模型，默认为\"BAAI/bge-en-icl\"。\n",
    "\n",
    "    返回:\n",
    "    dict: 包含嵌入的OpenAI API响应。\n",
    "    \"\"\"\n",
    "    # 使用指定模型为输入文本生成嵌入\n",
    "    response = client.embeddings.create(\n",
    "        model=model,\n",
    "        input=text\n",
    "    )\n",
    "\n",
    "    return response  # 返回包含嵌入的响应\n",
    "\n",
    "# 为文本片段生成嵌入\n",
    "response = create_embeddings(text_chunks)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 执行语义检索\n",
    "我们实现余弦相似度算法，以根据用户查询找到最相关的文本片段。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "    \"\"\"\n",
    "    计算两个向量的余弦相似度。\n",
    "\n",
    "    参数:\n",
    "    vec1 (np.ndarray): 第一个向量。\n",
    "    vec2 (np.ndarray): 第二个向量。\n",
    "\n",
    "    返回:\n",
    "    float: 两个向量的余弦相似度。\n",
    "    \"\"\"\n",
    "    # 计算两个向量的点积并除以它们的范数乘积\n",
    "    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def semantic_search(query, text_chunks, embeddings, k=5):\n",
    "    \"\"\"\n",
    "    使用查询和嵌入对文本片段进行语义检索。\n",
    "\n",
    "    参数:\n",
    "    query (str): 检索用的查询语句。\n",
    "    text_chunks (List[str]): 待检索的文本片段列表。\n",
    "    embeddings (List[dict]): 文本片段的嵌入列表。\n",
    "    k (int): 返回最相关片段的数量，默认为5。\n",
    "\n",
    "    返回:\n",
    "    List[str]: 前k个最相关的文本片段。\n",
    "    \"\"\"\n",
    "    # 为查询生成嵌入\n",
    "    query_embedding = create_embeddings(query).data[0].embedding\n",
    "    similarity_scores = []  # 初始化用于存储相似度分数的列表\n",
    "\n",
    "    # 计算查询嵌入与每个文本片段嵌入的相似度\n",
    "    for i, chunk_embedding in enumerate(embeddings):\n",
    "        similarity_score = cosine_similarity(np.array(query_embedding), np.array(chunk_embedding.embedding))\n",
    "        similarity_scores.append((i, similarity_score))  # 记录索引和相似度分数\n",
    "\n",
    "    # 按相似度分数降序排序\n",
    "    similarity_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    # 获取前k个最相似文本片段的索引\n",
    "    top_indices = [index for index, _ in similarity_scores[:k]]\n",
    "    # 返回前k个最相关的文本片段\n",
    "    return [text_chunks[index] for index in top_indices]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 在提取的片段上运行查询"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 从JSON文件加载验证数据\n",
    "with open('data/val.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 获取验证数据中的第一个查询\n",
    "query = data[0]['question']\n",
    "\n",
    "# 执行语义检索，获取与查询最相关的前2个文本片段\n",
    "top_chunks = semantic_search(query, text_chunks, response.data, k=2)\n",
    "\n",
    "# 打印查询内容\n",
    "print(\"Query:\", query)\n",
    "\n",
    "# 打印前2个最相关的文本片段\n",
    "for i, chunk in enumerate(top_chunks):\n",
    "    print(f\"Context {i + 1}:\\n{chunk}\\n=====================================\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 基于检索到的片段生成回答"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义AI助手的系统提示词\n",
    "system_prompt = \"You are an AI assistant that strictly answers based on the given context. If the answer cannot be derived directly from the provided context, respond with: 'I do not have enough information to answer that.'\"\n",
    "\n",
    "def generate_response(system_prompt, user_message, model=\"meta-llama/Llama-3.2-3B-Instruct\"):\n",
    "    \"\"\"\n",
    "    基于系统提示词和用户消息生成AI模型的回答。\n",
    "\n",
    "    参数:\n",
    "    system_prompt (str): 指导AI行为的系统提示词。\n",
    "    user_message (str): 用户消息或查询。\n",
    "    model (str): 用于生成回答的模型，默认为\"meta-llama/Llama-2-7B-chat-hf\"。\n",
    "\n",
    "    返回:\n",
    "    dict: AI模型的响应。\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# 基于top_chunks生成用户提示词\n",
    "user_prompt = \"\\n\".join([f\"Context {i + 1}:\\n{chunk}\\n=====================================\\n\" for i, chunk in enumerate(top_chunks)])\n",
    "user_prompt = f\"{user_prompt}\\nQuestion: {query}\"\n",
    "\n",
    "# 生成AI回答\n",
    "ai_response = generate_response(system_prompt, user_prompt)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 评估AI回答\n",
    "我们将AI的回答与期望答案进行对比，并给出评分。"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 定义评估系统的系统提示词\n",
    "evaluate_system_prompt = \"You are an intelligent evaluation system tasked with assessing the AI assistant's responses. If the AI assistant's response is very close to the true response, assign a score of 1. If the response is incorrect or unsatisfactory in relation to the true response, assign a score of 0. If the response is partially aligned with the true response, assign a score of 0.5.\"\n",
    "\n",
    "# 组合用户查询、AI回答、真实答案和评估系统提示词，生成评估提示词\n",
    "evaluation_prompt = f\"User Query: {query}\\nAI Response:\\n{ai_response.choices[0].message.content}\\nTrue Response: {data[0]['ideal_answer']}\\n{evaluate_system_prompt}\"\n",
    "\n",
    "# 使用评估系统提示词和评估提示词生成评估结果\n",
    "evaluation_response = generate_response(evaluate_system_prompt, evaluation_prompt)\n",
    "\n",
    "# 打印评估结果\n",
    "print(evaluation_response.choices[0].message.content)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
